{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "from tqdm import tqdm\n",
    "from arabert.preprocess import ArabertPreprocessor\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train stance counts\n",
      " 2    5538\n",
      "1    1012\n",
      "0     438\n",
      "Name: stance, dtype: int64\n",
      "dev stance counts\n",
      " 2    804\n",
      "1    126\n",
      "0     70\n",
      "Name: stance, dtype: int64\n",
      "---------------------\n",
      "train category counts\n",
      " info_news       3616\n",
      "personal        1025\n",
      "celebrity        975\n",
      "plan             606\n",
      "unrelated        323\n",
      "others           167\n",
      "requests         112\n",
      "rumors            79\n",
      "advice            67\n",
      "restrictions      18\n",
      "Name: category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Read the datasets\n",
    "train = pd.read_csv('dataset/train.csv')\n",
    "train['stance'] += 1\n",
    "print('train stance counts\\n', train['stance'].value_counts())\n",
    "dev = pd.read_csv('dataset/dev.csv')\n",
    "dev['stance'] += 1\n",
    "dev['stance'].value_counts()\n",
    "print('dev stance counts\\n', dev['stance'].value_counts())\n",
    "print('---------------------')\n",
    "print('train category counts\\n', train['category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the train dataset, there are 4 different versions made from it:\n",
    "    # 1. The original dataset with original counts\n",
    "    # 2. The dataset but with 500 tweets per class\n",
    "    # 3. The dataset but with 1000 tweets per class\n",
    "    # 4. The dataset but up 2500 tweets per class\n",
    "\n",
    "train_1 = train.copy()\n",
    "dev_1 = dev.copy()\n",
    "\n",
    "tmp1 = train[train['stance'] == 0].sample(500, random_state=42, replace=True)\n",
    "tmp2 = train[train['stance'] == 1].sample(500, random_state=42, replace=True)\n",
    "tmp3 = train[train['stance'] == 2].sample(500, random_state=42, replace=True)\n",
    "\n",
    "train_2 = pd.concat([tmp1, tmp2, tmp3])\n",
    "\n",
    "tmp1 = train[train['stance'] == 0].sample(1000, random_state=42, replace=True)\n",
    "tmp2 = train[train['stance'] == 1].sample(1000, random_state=42, replace=True)\n",
    "tmp3 = train[train['stance'] == 2].sample(1000, random_state=42, replace=True)\n",
    "\n",
    "train_3 = pd.concat([tmp1, tmp2, tmp3])\n",
    "\n",
    "tmp1 = train[train['stance'] == 0].sample(2500, random_state=42, replace=True)\n",
    "tmp2 = train[train['stance'] == 1].sample(2500, random_state=42, replace=True)\n",
    "tmp3 = train[train['stance'] == 2].sample(2500, random_state=42, replace=True)\n",
    "\n",
    "train_4 = pd.concat([tmp1, tmp2, tmp3])\n",
    "\n",
    "def equalize_dataset_category(dataset, count):\n",
    "    tmp1 = train[train['category'] == 'info_news'].sample(50, random_state=42, replace=True)\n",
    "    tmp2 = train[train['category'] == 'personal'].sample(50, random_state=42, replace=True)\n",
    "    tmp3 = train[train['category'] == 'celebrity'].sample(50, random_state=42, replace=True)\n",
    "    tmp4 = train[train['category'] == 'plan'].sample(50, random_state=42, replace=True)\n",
    "    tmp5 = train[train['category'] == 'unrelated'].sample(50, random_state=42, replace=True)\n",
    "    tmp6 = train[train['category'] == 'others'].sample(50, random_state=42, replace=True)\n",
    "    tmp7 = train[train['category'] == 'requests'].sample(50, random_state=42, replace=True)\n",
    "    tmp8 = train[train['category'] == 'rumors'].sample(50, random_state=42, replace=True)\n",
    "    tmp9 = train[train['category'] == 'advice'].sample(50, random_state=42, replace=True)\n",
    "    tmp10 = train[train['category'] == 'restrictions'].sample(50, random_state=42, replace=True)\n",
    "    return pd.concat([tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7, tmp8, tmp9, tmp10])\n",
    "\n",
    "# sample the train dataset such that every category contains 50 tweets\n",
    "train_5 = equalize_dataset_category(train, 50)\n",
    "\n",
    "# sample the train dataset such that every category contains 100 tweets\n",
    "train_6 = equalize_dataset_category(train, 100)\n",
    "\n",
    "# sample the train dataset such that every category contains 150 tweets\n",
    "train_7 = equalize_dataset_category(train, 150)\n",
    "\n",
    "# sample the train dataset such that every category contains 250 tweets\n",
    "train_8 = equalize_dataset_category(train, 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Some pytorch preparations\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('device:', device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Arabert (No stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-12-20 17:23:41,072 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
      "Some weights of the model checkpoint at aubmindlab/bert-base-arabertv2 were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# The first preprocessing pipeline, using the arabert model\n",
    "# This pipeline produces 2 outputs for every dataset:\n",
    "    # 1. tokenized data for every tweet --> This is so this data can be used by others for feature extraction\n",
    "    # 2. word embeddings for every tweet --> This can directly be used by the model\n",
    "\n",
    "model_name = \"aubmindlab/bert-base-arabertv2\"\n",
    "arabert_prep = ArabertPreprocessor(model_name=model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "bert_model = BertModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "def preprocess_arabert(text, embedding=True):\n",
    "    \"\"\"\n",
    "    This function preprocesses the text using arabert.\n",
    "    It's essentially a full pipeline that even returns the word embeddings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text: str\n",
    "        The text to be preprocessed\n",
    "    embedding: bool\n",
    "        Whether to return the word embeddings or not\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    output: list\n",
    "        The preprocessed text\n",
    "    \"\"\"\n",
    "\n",
    "    def clean_text(text):\n",
    "        # remove any word with + in it\n",
    "        text = re.sub(r'\\S*\\+\\S*', '', text)\n",
    "        # remove non arabic characters\n",
    "        text = re.sub(r'[^\\u0600-\\u06FF]', ' ', text)\n",
    "        # remove extra spaces\n",
    "        return text\n",
    "\n",
    "    output = arabert_prep.preprocess(text)\n",
    "    output = clean_text(output)\n",
    "    tokenized = tokenizer.tokenize(output)\n",
    "\n",
    "    if embedding:\n",
    "        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized)\n",
    "        tokens_tensor = torch.tensor([indexed_tokens]).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = bert_model(tokens_tensor)\n",
    "            encoded_layers = outputs[0]\n",
    "            encoded_layers = encoded_layers.view(-1, 768)\n",
    "        del tokens_tensor\n",
    "        del outputs\n",
    "        return encoded_layers\n",
    "    else:\n",
    "        return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6988/6988 [00:19<00:00, 362.28it/s]\n",
      "100%|██████████| 6988/6988 [01:33<00:00, 74.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# Use the first preprocessing pipeline to preprocess the datasets\n",
    "tqdm.pandas()\n",
    "\n",
    "train_1['tokenized'] = train_1['text'].progress_apply(lambda x: preprocess_arabert(x, embedding=False))\n",
    "train_1['embeddings'] = train_1['text'].progress_apply(lambda x: preprocess_arabert(x, embedding=True))\n",
    "\n",
    "# Write the datasets to pickle files -- So that it can be used by any other files easily\n",
    "train_1.to_pickle('output/train_1_arabert.pkl')\n",
    "\n",
    "# Can later be read using the following code:\n",
    "    # train_1 = pd.read_pickle('output/train_1.pkl')\n",
    "# The dimensions for every word is (1, 768)\n",
    "# This means that every sentence will have a dimension of (sentence_length, 768)\n",
    "\n",
    "# clear the memory\n",
    "del train_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [00:04<00:00, 373.85it/s]\n",
      "100%|██████████| 1500/1500 [00:20<00:00, 74.81it/s]\n"
     ]
    }
   ],
   "source": [
    "train_2['tokenized'] = train_2['text'].progress_apply(lambda x: preprocess_arabert(x, embedding=False))\n",
    "train_2['embeddings'] = train_2['text'].progress_apply(lambda x: preprocess_arabert(x, embedding=True))\n",
    "train_2.to_pickle('output/train_2_arabert.pkl')\n",
    "del train_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:08<00:00, 359.25it/s]\n",
      "100%|██████████| 3000/3000 [00:41<00:00, 73.10it/s]\n"
     ]
    }
   ],
   "source": [
    "train_3['tokenized'] = train_3['text'].progress_apply(lambda x: preprocess_arabert(x, embedding=False))\n",
    "train_3['embeddings'] = train_3['text'].progress_apply(lambda x: preprocess_arabert(x, embedding=True))\n",
    "train_3.to_pickle('output/train_3_arabert.pkl')\n",
    "del train_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7500/7500 [00:19<00:00, 380.35it/s]\n",
      "100%|██████████| 7500/7500 [01:37<00:00, 77.08it/s]\n"
     ]
    }
   ],
   "source": [
    "train_4['tokenized'] = train_4['text'].progress_apply(lambda x: preprocess_arabert(x, embedding=False))\n",
    "train_4['embeddings'] = train_4['text'].progress_apply(lambda x: preprocess_arabert(x, embedding=True))\n",
    "train_4.to_pickle('output/train_4_arabert.pkl')\n",
    "del train_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:01<00:00, 372.02it/s]\n",
      "100%|██████████| 500/500 [00:06<00:00, 74.66it/s]\n"
     ]
    }
   ],
   "source": [
    "train_5['tokenized'] = train_5['text'].progress_apply(lambda x: preprocess_arabert(x, embedding=False))\n",
    "train_5['embeddings'] = train_5['text'].progress_apply(lambda x: preprocess_arabert(x, embedding=True))\n",
    "train_5.to_pickle('output/train_5_arabert.pkl')\n",
    "del train_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:01<00:00, 337.36it/s]\n",
      "100%|██████████| 500/500 [00:06<00:00, 72.71it/s]\n"
     ]
    }
   ],
   "source": [
    "train_6['tokenized'] = train_6['text'].progress_apply(lambda x: preprocess_arabert(x, embedding=False))\n",
    "train_6['embeddings'] = train_6['text'].progress_apply(lambda x: preprocess_arabert(x, embedding=True))\n",
    "train_6.to_pickle('output/train_6_arabert.pkl')\n",
    "del train_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:01<00:00, 374.84it/s]\n",
      "100%|██████████| 500/500 [00:06<00:00, 75.39it/s]\n"
     ]
    }
   ],
   "source": [
    "train_7['tokenized'] = train_7['text'].progress_apply(lambda x: preprocess_arabert(x, embedding=False))\n",
    "train_7['embeddings'] = train_7['text'].progress_apply(lambda x: preprocess_arabert(x, embedding=True))\n",
    "train_7.to_pickle('output/train_7_arabert.pkl')\n",
    "del train_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:01<00:00, 388.64it/s]\n",
      "100%|██████████| 500/500 [00:06<00:00, 75.74it/s]\n"
     ]
    }
   ],
   "source": [
    "train_8['tokenized'] = train_8['text'].progress_apply(lambda x: preprocess_arabert(x, embedding=False))\n",
    "train_8['embeddings'] = train_8['text'].progress_apply(lambda x: preprocess_arabert(x, embedding=True))\n",
    "train_8.to_pickle('output/train_8_arabert.pkl')\n",
    "del train_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:02<00:00, 407.07it/s]\n",
      "100%|██████████| 1000/1000 [00:13<00:00, 76.09it/s]\n"
     ]
    }
   ],
   "source": [
    "dev_1['tokenized'] = dev_1['text'].progress_apply(lambda x: preprocess_arabert(x, embedding=False))\n",
    "dev_1['embeddings'] = dev_1['text'].progress_apply(lambda x: preprocess_arabert(x, embedding=True))\n",
    "dev_1.to_pickle('output/dev_1_arabert.pkl')\n",
    "del dev_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the bert_model and the arabert_prep & tokenizer\n",
    "del bert_model\n",
    "del arabert_prep\n",
    "del tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using our function (uses stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is made because the process of cleaning arabic text is complex\n",
    "# and depends on many unicodes done in many steps\n",
    "def clean_arabic(text):\n",
    "    # ! to understand this nonsense you need this link open \n",
    "    \"\"\" https://ar.wikipedia.org/wiki/%D8%A7%D9%84%D8%AE%D8%B7_\n",
    "        %D8%A7%D9%84%D8%B9%D8%B1%D8%A8%D9%8A_%D9%81%D9%8A_%D9%8A%D9\n",
    "        %88%D9%86%D9%8A%D9%83%D9%88%D8%AF   \"\"\"\n",
    "\n",
    "    # remove التشكيل\n",
    "    text = re.sub(r'[\\u0600-\\u061F]', '', text)\n",
    "    text = re.sub(r'[\\u064B-\\u066D]', '', text)\n",
    "\n",
    "    # Because of all the idiots that were bypassing twitters' spam filters using\n",
    "    # special characters like this idiot: كو.ر.ونا We'll remove all the special \n",
    "    # before everything else\n",
    "\n",
    "    # remove special characters\n",
    "    text = re.sub(r'[\\u0024-\\u003F]', '', text)\n",
    "    text = re.sub(r'[\\u005B-\\u0060]', '', text)\n",
    "    text = re.sub(r'[\\u007B-\\u007E]', '', text)\n",
    "\n",
    "    # replace weird characters with more standard ones\n",
    "    # 1. replace چ with ج\n",
    "    text = re.sub(r'چ','ج',text)\n",
    "\n",
    "    # 2. replace ڤ ڨ with ف\n",
    "    text = re.sub(r'ڤ','ف',text)\n",
    "    text = re.sub(r'ڨ','ف',text)\n",
    "\n",
    "    # 3. replace ڠ with ق\n",
    "    text = re.sub(r'ڠ','غ',text)\n",
    "    \n",
    "    # 4. replace ٱ\tٲ\tٳ\t◌ٴ\tٵ with ا\n",
    "    string = ['ٱ','ٲ','ٳ','ٴ','ٵ', 'آ', 'أ', 'إ']\n",
    "    for char in string:\n",
    "        text = re.sub(char,'ا',text)\n",
    "\n",
    "    # 5. replace ٶ\tٷ with و\n",
    "    string = ['ٶ','ٷ']\n",
    "    for char in string:\n",
    "        text = re.sub(char,'و',text)\n",
    "\n",
    "    # 6. replace ٸ ی with ي\n",
    "    text = re.sub(r'ٸ','ي',text) \n",
    "    text = re.sub(r'ی','ي',text)\n",
    "    \n",
    "    # 7. replace پ\twith ب\n",
    "    text = re.sub(r'پ','ب',text)\n",
    "\n",
    "    # 8. replace ژ with ز\n",
    "    text = re.sub(r'ژ','ز',text)\n",
    "\n",
    "    # 9. replace ک ڪ ګ ڬ ڭ ڮ گ ڰ ڱ ڲ ڳ ڴ with ك\n",
    "    string = ['ک', 'ڪ', 'ګ', 'ڬ', 'ڭ', 'ڮ', 'گ', 'ڰ', 'ڱ', 'ڲ', 'ڳ', 'ڴ']\n",
    "    for char in string:\n",
    "        text = re.sub(char,'ك',text)\n",
    "    # 10. replace ھ with ه\n",
    "    text = re.sub(r'ھ','ه',text)\n",
    "\n",
    "    # remove all extra arabic characters (shift + ت) \n",
    "    text = re.sub(r'ـ','',text)\n",
    "\n",
    "    # remove non arabic characters\n",
    "    text = re.sub(r'[^\\u0620-\\u064A\\s]',' ',text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\" removes all non arabic characters & replaces all spaces with a single space \"\"\"\n",
    "    \n",
    "    # remove all words with # in them\n",
    "    text = re.sub(r'[^\\s]*#[^\\s]*',' ',text)\n",
    "    \n",
    "    # arabic letters clean up \n",
    "    text = clean_arabic(text)\n",
    "        \n",
    "    # replace all white spaces with a single space\n",
    "    text = re.sub(r'\\s+',' ',text)\n",
    "    \n",
    "    return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def original_preprocess(dataset):\n",
    "    tqdm.pandas()\n",
    "    dataset['cleaned_data'] = dataset['text'].progress_apply(lambda x: clean_text(x))\n",
    "    dataset['tokenized_data'] = dataset['cleaned_data'].progress_apply(lambda x: nltk.word_tokenize(x))\n",
    "    dataset.drop(['cleaned_data'], axis=1, inplace=True)\n",
    "\n",
    "    # remove stopwords\n",
    "    stopwords = nltk.corpus.stopwords.words('arabic')\n",
    "\n",
    "    # Here we're looking for more stopwords that are 2 characters or less\n",
    "    # we spend hours doing just this for two or three character words\n",
    "    stopwords += ['ال', 'اي', 'ان', 'تم', 'بن', \n",
    "                'او', 'اي', 'عم', 'ام', 'رض',\n",
    "                'في', 'فى', 'رب', 'سم', 'خط',\n",
    "                'ول', 'زي', 'دي', 'اذ', 'ده',\n",
    "                'دى', 'انه', 'ابو', 'احد']\n",
    "    dataset['tokens_no_stopwords'] = dataset['tokenized_data'].progress_apply(lambda x: [word for word in x if word not in stopwords]) \n",
    "    dataset.drop(['tokenized_data'], axis=1, inplace=True)\n",
    "    dataset['tokens'] = dataset['tokens_no_stopwords'].progress_apply(lambda x: [ISRIStemmer().stem(word) for word in x])\n",
    "    dataset.drop(['tokens_no_stopwords'], axis=1, inplace=True)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the train dataset, there are 4 different versions made from it:\n",
    "    # 1. The original dataset with original counts\n",
    "    # 2. The dataset but with 500 tweets per class\n",
    "    # 3. The dataset but with 1000 tweets per class\n",
    "    # 4. The dataset but up 2500 tweets per class\n",
    "\n",
    "train_1 = train.copy()\n",
    "dev_1 = dev.copy()\n",
    "\n",
    "tmp1 = train[train['stance'] == 0].sample(500, random_state=42, replace=True)\n",
    "tmp2 = train[train['stance'] == 1].sample(500, random_state=42, replace=True)\n",
    "tmp3 = train[train['stance'] == 2].sample(500, random_state=42, replace=True)\n",
    "\n",
    "train_2 = pd.concat([tmp1, tmp2, tmp3]).sort_index()\n",
    "\n",
    "tmp1 = train[train['stance'] == 0].sample(1000, random_state=42, replace=True)\n",
    "tmp2 = train[train['stance'] == 1].sample(1000, random_state=42, replace=True)\n",
    "tmp3 = train[train['stance'] == 2].sample(1000, random_state=42, replace=True)\n",
    "\n",
    "train_3 = pd.concat([tmp1, tmp2, tmp3]).sort_index()\n",
    "\n",
    "tmp1 = train[train['stance'] == 0].sample(2500, random_state=42, replace=True)\n",
    "tmp2 = train[train['stance'] == 1].sample(2500, random_state=42, replace=True)\n",
    "tmp3 = train[train['stance'] == 2].sample(2500, random_state=42, replace=True)\n",
    "\n",
    "train_4 = pd.concat([tmp1, tmp2, tmp3]).sort_index()\n",
    "\n",
    "def equalize_dataset_category(dataset, count):\n",
    "    tmp1 = train[train['category'] == 'info_news'].sample(50, random_state=42, replace=True)\n",
    "    tmp2 = train[train['category'] == 'personal'].sample(50, random_state=42, replace=True)\n",
    "    tmp3 = train[train['category'] == 'celebrity'].sample(50, random_state=42, replace=True)\n",
    "    tmp4 = train[train['category'] == 'plan'].sample(50, random_state=42, replace=True)\n",
    "    tmp5 = train[train['category'] == 'unrelated'].sample(50, random_state=42, replace=True)\n",
    "    tmp6 = train[train['category'] == 'others'].sample(50, random_state=42, replace=True)\n",
    "    tmp7 = train[train['category'] == 'requests'].sample(50, random_state=42, replace=True)\n",
    "    tmp8 = train[train['category'] == 'rumors'].sample(50, random_state=42, replace=True)\n",
    "    tmp9 = train[train['category'] == 'advice'].sample(50, random_state=42, replace=True)\n",
    "    tmp10 = train[train['category'] == 'restrictions'].sample(50, random_state=42, replace=True)\n",
    "    return pd.concat([tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7, tmp8, tmp9, tmp10])\n",
    "\n",
    "# sample the train dataset such that every category contains 50 tweets\n",
    "train_5 = equalize_dataset_category(train, 50)\n",
    "\n",
    "# sample the train dataset such that every category contains 100 tweets\n",
    "train_6 = equalize_dataset_category(train, 100)\n",
    "\n",
    "# sample the train dataset such that every category contains 150 tweets\n",
    "train_7 = equalize_dataset_category(train, 150)\n",
    "\n",
    "# sample the train dataset such that every category contains 250 tweets\n",
    "train_8 = equalize_dataset_category(train, 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6988/6988 [00:00<00:00, 9920.50it/s]\n",
      "100%|██████████| 6988/6988 [00:01<00:00, 5162.09it/s]\n",
      "100%|██████████| 6988/6988 [00:01<00:00, 3715.64it/s]\n",
      "100%|██████████| 6988/6988 [00:01<00:00, 4844.84it/s]\n",
      "100%|██████████| 1500/1500 [00:00<00:00, 10748.31it/s]\n",
      "100%|██████████| 1500/1500 [00:00<00:00, 5456.33it/s]\n",
      "100%|██████████| 1500/1500 [00:00<00:00, 3580.75it/s]\n",
      "100%|██████████| 1500/1500 [00:00<00:00, 4549.11it/s]\n",
      "100%|██████████| 3000/3000 [00:00<00:00, 10380.84it/s]\n",
      "100%|██████████| 3000/3000 [00:00<00:00, 5115.93it/s]\n",
      "100%|██████████| 3000/3000 [00:00<00:00, 3505.93it/s]\n",
      "100%|██████████| 3000/3000 [00:00<00:00, 4636.00it/s]\n",
      "100%|██████████| 7500/7500 [00:00<00:00, 10523.28it/s]\n",
      "100%|██████████| 7500/7500 [00:01<00:00, 5302.02it/s]\n",
      "100%|██████████| 7500/7500 [00:02<00:00, 3714.68it/s]\n",
      "100%|██████████| 7500/7500 [00:01<00:00, 4699.55it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 8992.24it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 5272.91it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 3925.37it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 5223.67it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 11506.50it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 6013.91it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 3940.24it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 4184.12it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 10339.10it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 5379.70it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 3953.03it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 4770.55it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 11379.51it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 5423.15it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 3749.91it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 4328.67it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 11199.15it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 5633.28it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3978.79it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 5176.29it/s]\n"
     ]
    }
   ],
   "source": [
    "train_1 = original_preprocess(train_1)\n",
    "train_2 = original_preprocess(train_2)\n",
    "train_3 = original_preprocess(train_3)\n",
    "train_4 = original_preprocess(train_4)\n",
    "train_5 = original_preprocess(train_5)\n",
    "train_6 = original_preprocess(train_6)\n",
    "train_7 = original_preprocess(train_7)\n",
    "train_8 = original_preprocess(train_8)\n",
    "dev_1 = original_preprocess(dev_1)\n",
    "\n",
    "# save the datasets to pickle files\n",
    "train_1.to_pickle('output/train_1_original.pkl')\n",
    "train_2.to_pickle('output/train_2_original.pkl')\n",
    "train_3.to_pickle('output/train_3_original.pkl')\n",
    "train_4.to_pickle('output/train_4_original.pkl')\n",
    "train_5.to_pickle('output/train_5_original.pkl')\n",
    "train_6.to_pickle('output/train_6_original.pkl')\n",
    "train_7.to_pickle('output/train_7_original.pkl')\n",
    "train_8.to_pickle('output/train_8_original.pkl')\n",
    "dev_1.to_pickle('output/dev_1_original.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "0de4e91a703ed839300569d00f5f7f96e515c5b82ac00b296860b354eb12ca4a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
